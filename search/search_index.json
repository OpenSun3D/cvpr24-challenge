{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":""},{"location":"#workshop-challenge","title":"Workshop Challenge","text":""},{"location":"track_1/","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":"Challenge Track 1: Open-vocabulary 3D object instance search"},{"location":"track_1/#overview","title":"Overview","text":"The ability to perceive, understand and interact with arbitrary 3D environments is a long-standing research goal with applications in AR/VR, robotics, health and industry. Many 3D scene understanding methods are largely limited to recognizing a closed-set of pre-defined object classes. In the first track of our workshop challenge, we focus on open-vocabulary 3D object instance search. Given a 3D scene and an open-vocabulary, text-based query, the goal is to localize and densely segment all object instances that fit best with the specified query. If there are multiple objects that fit the given prompt, each of these objects should be segmented, and labeled as separate instances. The list of queries can refer to long-tail objects, or can include descriptions of object properties such as semantics, material type, and situational context."},{"location":"track_2/","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":"Challenge Track 2: Open-vocabulary 3D affordance grounding"},{"location":"track_2/#overview","title":"Overview","text":"<p>Lorem ipsum</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":""},{"location":"#workshop-challenge","title":"Workshop Challenge","text":""},{"location":"track_1/","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":"Challenge Track 1: Open-vocabulary 3D object instance search"},{"location":"track_1/#overview","title":"Overview","text":"The ability to perceive, understand and interact with arbitrary 3D environments is a long-standing research goal with applications in AR/VR, robotics, health and industry. Many 3D scene understanding methods are largely limited to recognizing a closed-set of pre-defined object classes. In the first track of our workshop challenge, we focus on open-vocabulary 3D object instance search. Given a 3D scene and an open-vocabulary, text-based query, the goal is to localize and densely segment all object instances that fit best with the specified query. If there are multiple objects that fit the given prompt, each of these objects should be segmented, and labeled as separate instances. The list of queries can refer to long-tail objects, or can include descriptions of object properties such as semantics, material type, and situational context."},{"location":"track_1/#tentative-dates","title":"Tentative dates","text":"<ul> <li>Submission Portal: EvalAI</li> <li>Data Instructions &amp; Helper Scripts: April 15, 2024</li> <li>Dev Phase Start: April 15, 2024</li> <li>Submission Portal Start: April 15, 2024</li> <li>Test Phase Start: May 1, 2024</li> <li>Test Phase End: June 8, 2024 (23:59 Pacific Time)</li> </ul>"},{"location":"track_2/","title":"2nd Workshop on Open-Vocabulary 3D Scene Understanding","text":"Challenge Track 2: Open-vocabulary 3D affordance grounding"},{"location":"track_2/#overview","title":"Overview","text":"Most existing methods in 3D scene understanding are heavily focused on understanding the scene on an object level by detecting or segmenting the 3D object instances. However, identifying 3D objects is only an intermediate step towards a more fine-grained goal. In real-world applications, agents need to successfully detect and interact with the functional interactive elements in the scene, such as knobs, handles and buttons, and reason about their purpose in the scene context. Through interacting with these elements, agents can accomplish diverse tasks, such as opening a drawer or turning on the light. In the second track of our workshop challenge, we focus on open-vocabulary 3D affordance grounding. Given a 3D scene and an open-vocabulary, text-based description of a task (e.g., \"open the fridge\"), the goal is to segment the functional interactive element that the agent needs to interact with (e.g., fridge handle) to successfully accomplish the task."},{"location":"track_2/#tentative-dates","title":"Tentative dates","text":"<ul> <li>Submission Portal: EvalAI</li> <li>Data Instructions &amp; Helper Scripts: April 15, 2024</li> <li>Dev Phase Start: April 15, 2024</li> <li>Submission Portal Start: April 15, 2024</li> <li>Test Phase Start: May 1, 2024</li> <li>Test Phase End: June 8, 2024 (23:59 Pacific Time)</li> </ul>"}]}
# 2nd Workshop on Open-Vocabulary 3D Scene Understanding 

<h2><strong>Challenge Track 1</strong>: Open-vocabulary 3D object instance search</h2>

<!-- ![Alt text](assets/scenefun3d_2.png "a title") -->
<!-- <video controls autoplay loop poster="assets/teaser2_poster.png">
  <source src="assets/teaser2.mp4" type="video/mp4">
</video> -->

<!-- ![Track 1 teaser](assets/track1_teaser.png) -->
<p align="center">
<img src="/cvpr24-challenge/assets/track1_teaser.png" alt="Track 1 teaser" width="620"/>
</p>


## Overview 

<div style="text-align: justify">
The ability to perceive, understand and interact with arbitrary 3D environments is a long-standing research goal with applications in AR/VR, robotics, health and industry. Many 3D scene understanding methods are largely limited to recognizing a closed-set of pre-defined object classes. In the first track of our workshop challenge, we focus on open-vocabulary 3D object instance search. Given a 3D scene and an open-vocabulary, text-based query, the goal is to localize and densely segment all object instances that fit best with the specified query. If there are multiple objects that fit the given prompt, each of these objects should be segmented, and labeled as separate instances. The list of queries can refer to long-tail objects, or can include descriptions of object properties such as semantics, material type, and situational context.
</div>
<!-- * `mkdocs new [dir-name]` - Create a new project.
* `mkdocs serve` - Start the live-reloading docs server.
* `mkdocs build` - Build the documentation site.
* `mkdocs -h` - Print help message and exit. -->

<!-- ## Data download

    mkdocs.yml    # The configuration file.
    docs/
        index.md  # The documentation homepage.
        ...       # Other markdown pages, images and other files.


## Submission instructions


## Evaluation guidelines -->